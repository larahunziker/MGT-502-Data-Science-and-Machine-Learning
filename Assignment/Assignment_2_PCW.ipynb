{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dc4475-7f7d-4457-8762-bb031a0629b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from surprise.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import get_dataset_dir\n",
    "from surprise import Dataset\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage \n",
    "from sklearn.cluster import AgglomerativeClustering  \n",
    "from sklearn.cluster import KMeans \n",
    "import time\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as plt_clrs\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a8c0-078c-4946-8927-e570469c601c",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "Welcome to the second assignment! \n",
    "\n",
    "You will have to implement clustering, association rules, and recommender systems algorithms, applying these methods to: \n",
    "- explore the similarities within groups of people watching movies (clustering analysis)\n",
    "- discover the relations between movies genre (association rules)\n",
    "- recommend movies to users (recommender system)\n",
    "\n",
    "We will use the MovieLens dataset, which contains movie ratings collected from the MovieLens website by the [GroupLens](https://grouplens.org/) research lab.\n",
    "\n",
    "Source: F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. *ACM Transactions on Interactive Intelligent Systems (TiiS)* 5, 4: 19:1–19:19. <https://doi.org/10.1145/2827872>\n",
    "\n",
    "Once you are done you have to submit your notebook here: \n",
    "[https://moodle.epfl.ch/mod/assign/view.php?id=1247726](https://moodle.epfl.ch/mod/assign/view.php?id=1247726)\n",
    "\n",
    "If there is need for further clarifications on the questions, after the assignment is released, we will update this file, so make sure you check the git repository for updates.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306ac928-a1e5-4dd7-8e62-b8108c22e5a9",
   "metadata": {},
   "source": [
    "## Clustering analysis: similarities between people (10 points)\n",
    "\n",
    "In this section, you will try to form clusters of individuals based on their preferences regarding movie genres. You will use a transformed version of the MovieLens dataset containing, for a selection of users:\n",
    "- their average rating of all science fiction movies they rated,\n",
    "- their average rating of all comedy movies they rated.\n",
    "\n",
    "Better understanding the differences in people's tastes can help improve the design of recommender systems, for instance for the creation of the user neighborhood. Ok, let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beccfea-24ca-4c3f-92f1-c5a950b2cbce",
   "metadata": {},
   "source": [
    "- Load the data in a dataframe. The url link is provided below. Display the first 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87e292-3eed-4193-ba24-60693606b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_clustering = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/ratings_clustering.csv'\n",
    "movie_ratings_df = pd.read_csv(url_clustering)\n",
    "\n",
    "# Displaying 10 first observations\n",
    "display(movie_ratings_df.head(10))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34f9776b-3f5a-4738-9e27-5d86bc68f6bc",
   "metadata": {},
   "source": [
    "- Plot a dendogram using 'ward' as linkage method and 'euclidean' as metric. \n",
    "- Based on the dendogram, how many clusters do you think is optimal? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaa0de-e766-42a9-a412-878d417ff40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the linkage method we want and the chosen distance metric.\n",
    "X = movie_ratings_df.copy()\n",
    "\n",
    "method_Z = 'ward'\n",
    "metric = 'euclidean'\n",
    "Z = linkage(X, method=method_Z, metric=metric)\n",
    "\n",
    "# Single linkage\n",
    "plt.figure(figsize=(16, 6))\n",
    "dendrogram(Z)  # Plot the dendogram according the linkage\n",
    "plt.title('Dendrogram (Method: ' + method_Z +\n",
    "          ', Metric: ' + metric + ')', fontsize=14)\n",
    "plt.hlines(y=4.5, xmin=0, xmax=2000, colors='k', linestyles='--')\n",
    "plt.text(x=1000, y=4.75, s='Horizontal line crossing 4 vertical lines', fontsize=12)\n",
    "plt.xlabel('Index of observations', fontsize=14)\n",
    "plt.ylabel('Distance', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48bf6ef2",
   "metadata": {},
   "source": [
    "If we use the technique seen in the exercises we are looking for the longes vertical lines not crossed by a extended horizontal line. Implementing this technique on this dendrogram we can find four clusters. The four different color-schemes also supports these findings, and makes them easily detectable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783cb37-9721-436d-8915-abbde7c3ee17",
   "metadata": {},
   "source": [
    "- Implement the Elbow method to determine the optimum number of cluster for K-Means algorithm (use `random_state=17` as parameter of K-Means). \n",
    "- Based on the Elbow method, how many clusters do you think is optimal? Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bcfd7-156e-4168-a618-94a11ddf2bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate elbow of an inertia\n",
    "\n",
    "def get_elbow_cluster(inertias, staring_cluster=2):\n",
    "    '''\n",
    "    Input: Inertias and clusters\n",
    "    Output: Cluster-elbow\n",
    "    '''\n",
    "    data = pd.DataFrame(data=inertias)\t\t# Creating df with inertias\n",
    "    data['pre'] = data[0].shift(1)\t\t\t# Setting the previous values with shift 1\n",
    "    # Setting the succeeding values with shift -1\n",
    "    data['post'] = data[0].shift(-1)\n",
    "    data['change'] = (data['post'] - data[0]) + \\\n",
    "        (data['pre'] - data[0]) \t# Calculating the change\n",
    "    # Finding the datapoint with the highest change compared to the next\n",
    "    data['elbow'] = data['change'] - data['change'].shift(-1)\n",
    "\n",
    "    data.sort_values(by=['elbow'], ascending=False, inplace=True)\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    elbow_index = data['index'][0]+staring_cluster\n",
    "    elbow_inertia = data[0][0]\n",
    "\n",
    "    return elbow_index, elbow_inertia\n",
    "\n",
    "\n",
    "# Declaring starting variables\n",
    "n_clusters, start_time, previous_inertia, end_time, inertias = 2, time.time(), 100, 0, []\n",
    "\n",
    "# Using a while loop that runs for maximum 3 seconds or breaks when reduction becomes smaller than 7,5%\n",
    "while end_time-start_time < 3:\n",
    "    km = KMeans(n_clusters=n_clusters, random_state=17, n_init=10).fit(X)\n",
    "    if previous_inertia/km.inertia_ > 1.075:\n",
    "        inertias.append(km.inertia_)\n",
    "        end_time = time.time()\n",
    "        n_clusters += 1\n",
    "        previous_inertia = km.inertia_\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# Using helper function to get the 'optimal' number of clusters\n",
    "elbow_index, elbow_inertia = get_elbow_cluster(inertias)\n",
    "\n",
    "nbr_clusters = range(2, n_clusters)\n",
    "\n",
    "# Plot\n",
    "plt.plot(nbr_clusters, inertias, '-o')\n",
    "plt.xticks(nbr_clusters)\n",
    "plt.title('Elbow method for inertia')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.vlines(x=elbow_index, ymin=0, ymax=100, linestyles='--', color='k')\n",
    "plt.text(x=elbow_index+0.25, y=50,\n",
    "         s=f'Optimal number of clusters: {elbow_index}', fontsize=10)\n",
    "plt.text(elbow_index+1, elbow_inertia, f'Elbow Intertia: {round(elbow_inertia, 2)}') \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9592b12",
   "metadata": {},
   "source": [
    "We can see that after four clusters the reduction in inertia gets lower and lower. I implemented a function called `get_elbow_cluster()` to identify this elbow, where we find the cluster number with the highest difference in change from the previous to the next cluster. I also used a `while` loop to that either runs for a maximum of three seconds or stops when the inertia improvement becomes small (1.075), this way i don't need to specify number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380737c-27d5-4e77-b47f-fe955e0c3e52",
   "metadata": {},
   "source": [
    "- Implement (train) a K-Means algorithm with the number of clusters of your choice. Use `random_state=17` as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c937a-c278-49d6-8af0-c972172864a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 4\n",
    "\n",
    "# Create K-Means model\n",
    "kmeans_movie = KMeans(n_clusters=clusters, random_state=17,\n",
    "                       n_init='auto')  \n",
    "# Fit  model\n",
    "kmeans_movie.fit(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "088087b0-c0d5-4b14-b9f9-db39cf63652c",
   "metadata": {},
   "source": [
    "- Implement (train) a hierarchical algorithm with the same number of clusters as for the K-Means model. Use 'ward' as linkage method and 'euclidean' as metric/affinity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc05f2-78a3-4e36-bc9d-a9aef8a83249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agglomerative Hierarchical model\n",
    "agglomerative_movie = AgglomerativeClustering(n_clusters=clusters, metric='euclidean', linkage='ward')\n",
    "\n",
    "# Fit model\n",
    "agglomerative_movie.fit(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "936ecb5f-3c7c-469d-b504-98a6c6e8f082",
   "metadata": {},
   "source": [
    "- Create a figure consisting of two subplots:\n",
    "    - a scatterplot of 'avg_scifi_rating' and 'avg_comedy_rating' colored by the clusters predicted with your KMeans model. Add the cluster centers to your plot. Label your clusters with the name of your choice (e.g., 'Comedy aficionado').\n",
    "    - a scatterplot of 'avg_scifi_rating' and 'avg_comedy_rating' colored by the clusters predicted with your hierarchical algorithm model. Label your clusters with the name of your choice.\n",
    "- How do your models compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b19051-9b33-4ea4-8ef6-0851ee6c13e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a df to keep color schemes consistent\n",
    "df_kmean_agglo = movie_ratings_df.copy()\n",
    "df_kmean_agglo['kmeans_cluster'] = kmeans_movie.labels_\n",
    "df_kmean_agglo['agglo_cluster'] = agglomerative_movie.labels_\n",
    "\n",
    "# Extract columns of choice\n",
    "x = df_kmean_agglo['avg_scifi_rating']\n",
    "y = df_kmean_agglo['avg_comedy_rating']\n",
    "c_kmean = df_kmean_agglo['kmeans_cluster']\n",
    "c_agglo = df_kmean_agglo['agglo_cluster']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Define the legend labels for the different types of movie wathcers\n",
    "watcher_labels = ['Poor Comedy Watchers', 'Wannabe Comedians',\n",
    "                  'Star Wars Fans', 'Undecided']\n",
    "\n",
    "# Creating scatterplot for the KMean\n",
    "kmean_scatter = ax[0].scatter(x=x,\n",
    "                              y=y,\n",
    "                              c=c_kmean,\n",
    "                              cmap=plt_clrs.ListedColormap(['lightblue', 'darkblue', 'lightpink', 'lightsalmon']))\n",
    "\n",
    "# Creating markers for each KMean center\n",
    "ax[0].scatter(kmeans_movie.cluster_centers_[:, 0],\n",
    "              kmeans_movie.cluster_centers_[:, 1],\n",
    "              c='black',\n",
    "              marker='x')\n",
    "\n",
    "# Setting title and labels for KMean model\n",
    "ax[0].set_title(f'KMeans Model with {kmeans_movie.n_clusters} Clusters')\n",
    "ax[0].set_xlabel('Avg Sci-Fi Rating')\n",
    "ax[0].set_ylabel('Avg Comendy Rating')\n",
    "ax[0].legend(handles=kmean_scatter.legend_elements()[0], labels=watcher_labels,\n",
    "             loc='lower left', bbox_to_anchor=(-0.01, -0.01, 0.1, 0.1))\n",
    "\n",
    "\n",
    "# Creating scatter for agglomerative model\n",
    "agglo_scatter = ax[1].scatter(x=x,\n",
    "                              y=y,\n",
    "                              c=c_agglo,\n",
    "                              cmap=plt_clrs.ListedColormap(['lightsalmon', 'darkblue', 'lightpink', 'lightblue']))\n",
    "ax[1].set_title(\n",
    "    f'Agglomerative Model with {agglomerative_movie.n_clusters_} Clusters')\n",
    "ax[1].set_xlabel('Avg Sci-Fi Rating')\n",
    "ax[1].set_ylabel('Avg Comendy Rating')\n",
    "ax[1].legend(handles=kmean_scatter.legend_elements()[0], labels=watcher_labels,\n",
    "             loc='lower left', bbox_to_anchor=(-0.01, -0.01, 0.1, 0.1))\n",
    "\n",
    "plt.subplots_adjust(wspace=0.4)   # Space between plots\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35aced80",
   "metadata": {},
   "source": [
    "We can see that both models capture the four clusters relatively similar, with the biggest difference being the center data-points. To me it seems that the KMeans model  perform slightly better when only using 4 clusters, as we know that `AgglomerativeClustering` perform better bigger cluster numbers. We know that `AgglomerativeClustering` builds its clusters with a bottom-up approach, where `ward` represents the linkage method. We can see that the agglomerative model has the most uneven cluster sizes, where `Undecided` is the the biggest. This is due to its “rich get richer” behavior. `Ward` is the best strategy for obtaining the most regular cluster sizes, whereas single `linkage` is the worst. Since the data is not new or unseen, the transductive vs inductive difference of these two algorithms does not apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195ca1f-77d2-446f-a178-8665c213aef1",
   "metadata": {},
   "source": [
    "## Association Rules: association between movie genres (10 points)\n",
    "\n",
    "You will now pursue your analysis, but this time trying to dig out information about movies. More precisely, you will search for matches between film genres using association rules. We try to understand, for instance, how likely it is that a film is both drama and action. This information can be interesting for film producers who may either want to produce something similar to the established norm: if most drama films are also action, perhaps the new action-drama film would be equally appreciated, or quite to the contrary try a new combination of genres which is more rare to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb45c9e-c071-4dad-8011-445ff946d797",
   "metadata": {},
   "source": [
    "- Load the data in a dataframe. The url link is provided below. \n",
    "- Display the first 10 observations. \n",
    "- Print the unique values of genres from the first column. \n",
    "- How many unique genres does the first column contain? \n",
    "- How many movies does the dataframe contains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764800a3-1399-480f-abb3-d79609193866",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_association_rules = 'https://raw.githubusercontent.com/michalis0/MGT-502-Data-Science-and-Machine-Learning/main/data/movies_assoc_rules.csv'\n",
    "\n",
    "df_association_rules = pd.read_csv(url_association_rules)\n",
    "\n",
    "display(df_association_rules.head(10))\n",
    "\n",
    "column = 0\n",
    "unique_values_in_column = df_association_rules.iloc[:,column].unique()\n",
    "number_of_genres = len(unique_values_in_column)\n",
    "number_of_movies = df_association_rules.shape[0]\n",
    "\n",
    "print(f'The unique values of the first column are: \\n{unique_values_in_column}\\n')\n",
    "print(f'Then numbers of unique genres in the column are: {number_of_genres}')\n",
    "print(f'The dataset contains {number_of_movies} movies')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd9f12-a4a8-46ba-8cdb-ef0a3c9bf6c8",
   "metadata": {},
   "source": [
    "- Preprocessing: as seen during the lab, convert the dataset using a `Transaction Encoder` from the `mlextend` module so that the dataset is reorganised in columns of unique genres. Rows should contain only True or False boolean values according to whether a film was considered as belonging to a genre column or not. Check that you have the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16ef7d-81a3-4fbb-95bd-88f5a2d31302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe to list of list\n",
    "movie_list = df_association_rules.values.tolist()\n",
    "\n",
    "# Remove NaNs with list comprehensions\n",
    "movie_list_cleaned = [[x for x in y if str(x).lower() != 'nan'] for y in movie_list]\n",
    "\n",
    "# Create instance of Encoder\n",
    "te = TransactionEncoder()\n",
    "\n",
    "# Fit encoder and transform our list\n",
    "movie_list_encoded = te.fit(movie_list_cleaned).transform(movie_list_cleaned)\n",
    "\n",
    "# Create dataframe with results\n",
    "movies_encoded = pd.DataFrame(movie_list_encoded, columns=te.columns_)\n",
    "display(movies_encoded.head())\n",
    "\n",
    "unique_genres = df_association_rules.describe().loc['unique'].max()      # Number of unique genres\n",
    "unique_genres_encoded = len(movies_encoded.columns)                      # Number of genres\n",
    "number_of_movies_encoded = movies_encoded.shape[0]                       # Number of movies\n",
    "\n",
    "# Doing manual check to check if the data is consistent with each other\n",
    "if unique_genres_encoded == unique_genres:\n",
    "    print(f'The number of genres are the same in the database as in the encoded data ({unique_genres})')\n",
    "    \n",
    "else:\n",
    "    print(f'The number of genres are not the same ({unique_genres_encoded}!={unique_genres})')\n",
    "\n",
    "if number_of_movies == number_of_movies_encoded:\n",
    "    print(f'We have the same number of movies ({number_of_movies})')\n",
    "else:\n",
    "    print(f'We do not have the same number of movies ({number_of_movies}!={number_of_movies_encoded})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e0e99-7012-4efa-99e4-56eb9bd5eee5",
   "metadata": {},
   "source": [
    "- Frequent itemsets: using the Apriori algorithm to find the frequent itemsets with minimum support of 0.01. There is no condition on the maximum length of an itemset. \n",
    "- How many itemsets did the apriori algorithm return above (for min_support=0.01)? \n",
    "- What are the 10 itemsets with the largest support (you can directly display a dataframe with the 10 itemsets and their support)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242fe963-f30a-452c-9a64-9d74c3b8431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# Apriori algorithm\n",
    "freq_items = apriori(movies_encoded, min_support=threshold, use_colnames=True)\n",
    "\n",
    "# Sorting on the support column and displaying the first 10 itemsets\n",
    "display(freq_items.sort_values(by=['support'], ascending=False).head(10))\n",
    "print(f'{freq_items.shape[0]} are above the minimum support trehold of {threshold}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f14e3247-3a63-4f7a-b43a-16c59b95708a",
   "metadata": {},
   "source": [
    "- Mining for association rules: using the frequent items identified above, find association rules with a minimum confidence of 0.45 and order them by decreasing value of lift.\n",
    "- Discuss the following statements (true or false with 1-2 lines justification)\n",
    "    - Animation films are associated with Children.  \n",
    "    - If a film has the genre Musical, then it is also a Comedy.\n",
    "    - If War then Drama is the asociation rule with the highest confidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff18aa-cda1-4402-857f-1b31c95ddecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate rules with 0.45 as minimum confidence\n",
    "rules = association_rules(freq_items, metric='confidence', min_threshold=0.45)\n",
    "\n",
    "# Sorting rules by lift \n",
    "rules.sort_values(by='lift', ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b53625e8-c4f0-4146-a4be-2c03f8df1b2a",
   "metadata": {},
   "source": [
    "*Discuss statements here* \n",
    "- Animation films are associated with Children.  \n",
    "\t- True. The association rule between 'Animation' and 'Children' has a confidence of ~ 0.46, which indicates that there is an association between these two genres. The lift value of ~10.9 also suggests a high likelihood of occurrence of 'Children' genre when 'Animation' genre is present, further supporting the statement, even though the support of ~ 0.08 could be interpreted in the way that it's hard to generalize.\n",
    "- If a film has the genre Musical, then it is also a Comedy.\n",
    "\t- False. The association rule between 'Musical' and 'Comedy' has a confidence of ~0.48, which is not close to 1, indicating that there is only a moderate association between these two genres. Additionally, the lift value of ~1.53 is also not significantly high, suggesting that the presence of 'Musical' genre does not necessarily guarantee the presence of 'Comedy' genre. \n",
    "- If War then Drama is the asociation rule with the highest confidence.\n",
    "\t- True. The association rule between 'War' and 'Drama' has a confidence of ~0.75, which is the highest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08235a6-27ec-45b7-9adc-0eea440b6b02",
   "metadata": {},
   "source": [
    "## Recommender systems: item-based recommender system (10 points)\n",
    "\n",
    "In the walkthrough, we have implemented a user-to-user collaborative filtering algorithm (from scratch and using using Surprise library), i.e., our recommendations were based on the ratings of users with similar tastes. In this assignment, you will implement an **item-to-item** collaborative filtering algorithm, i.e., the recommendations will be based on the set of movies that users like. Do not worry, you won't have to implement the algorithm from scratch and instead can rely on the [Surprise library](http://surpriselib.com/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc548b9a-1a50-4ada-beb4-100ffe9bd701",
   "metadata": {},
   "source": [
    "- As in the walkthrough, load the *built-in* `ml-100k` from the Surprise library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678f71a-015e-4fd6-aae9-ec15ccc47b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = Dataset.load_builtin('ml-100k')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b550a4-6dd1-441a-8d55-ee8b3113a932",
   "metadata": {},
   "source": [
    "- Use GridSearchCV to find the best number of neighbors (k) for a KNNWithMeans **item-based** algorithm, with the following parameters:\n",
    "    - options for k: `[10, 20, 30, 40, 50]`\n",
    "    - `'sim_options': {'name': ['pearson'], 'user_based': [???]}` Here you have to replace `???` with the appropriate value...\n",
    "    - root-mean-square-error (RMSE) as measures,\n",
    "    - 5 cross-validation folds,\n",
    "    - other parameters: `refit=True, joblib_verbose=2, n_jobs=-1`\n",
    "- What is the optimal k for which GridSearchCV returned the best RMSE score? \n",
    "- What is the RMSE score for the optimal k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3971dda-842f-4091-9afe-cb186943e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'k': [10, 20, 30, 40, 50],\n",
    "              'sim_options': {'name': ['pearson'], 'user_based': [False]}}\n",
    "\n",
    "# Using GridSearcgCV to determine the best k\n",
    "KNN_grid_search = GridSearchCV(KNNWithMeans, param_grid=param_grid,\n",
    "                               measures=['RMSE'], cv=5,\n",
    "                               refit=True, joblib_verbose=2, n_jobs=-1)\n",
    "\n",
    "KNN_grid_search.fit(data)\n",
    "\n",
    "best_k = KNN_grid_search.best_params['rmse']['k']\n",
    "best_rmse = KNN_grid_search.best_score['rmse'].round(5)\n",
    "\n",
    "print(f'\\n\\nOptimal K: {best_k}')\n",
    "print(f'Best RMSE: {best_rmse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd98fcb-c389-4807-b50b-bf8d56e5404b",
   "metadata": {},
   "source": [
    "- Using the Surprise library, split your dataset between training and test set. As parameters, use `test_size=0.2, random_state=12`\n",
    "- Fit a KNNWithMeans algorithm using the best k value retrieved above. As other parameters, use:\n",
    "    - `min_k=1`\n",
    "    - `sim_options = {'name': 'pearson','user_based': ???}`\n",
    "    - `verbose=False`\n",
    "- Predict ratings on the test set using your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b342582d-44a9-486b-bcf5-6a58000487a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_options = {\n",
    "    'name': 'pearson',\n",
    "    'user_based': False     # user_based = False to capture item-based\n",
    "}\n",
    "\n",
    "# Create training and test set\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=12)\n",
    "\n",
    "# Instance of KNNWithMeans\n",
    "knn_means = KNNWithMeans(k=best_k, min_k=1, sim_options=sim_options, verbose=False)\n",
    "\n",
    "# Fit model on training set\n",
    "knn_means.fit(trainset)\n",
    "\n",
    "# Predict ratings on test set\n",
    "predictions = knn_means.test(testset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9250babc-8ba9-4234-9d21-429766d34bd4",
   "metadata": {},
   "source": [
    "- Use the helper function below to identify the best 10 films for all users\n",
    "- Find the top 10 predictions for user 169 (you should return the titles of the movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1156-6120-416a-a03e-4b126d2964f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_item_names():\n",
    "    '''Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    '''\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    '''Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    '''\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list) # This is used to group a sequence of key-value pairs into a dictionary of lists\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "    return top_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f88a04-9eb7-49da-90ea-0f914124cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 movies for all users\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Getting mappings from row id to movie name and vice-versa\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "uid = '169'\n",
    "\n",
    "# Getting the top 10 movies for uid=169\n",
    "top_n_movie_ids_for_uid = top_n[uid]\n",
    "\n",
    "# Getting the first element in each pair (corresponding to each movie id)\n",
    "recommended_items = [iid for (iid, _) in top_n_movie_ids_for_uid]\n",
    "\n",
    "# Convert ids into names\n",
    "item_names = [rid_to_name[rid]\n",
    "              for rid in recommended_items]\n",
    "\n",
    "# Print the recommended items for user id 169\n",
    "print(f'\\nTop 10 movies for user with id {uid}:')\n",
    "print('\\n'.join(item_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0093ba6-a590-4969-bd78-5fa06b4f51b5",
   "metadata": {},
   "source": [
    "- Plot the precision at rank k and the recall at rank k on the same figure, for k between 0 and 20, and a relevance threshold of 3.75\n",
    "- Plot the precision-recall curve\n",
    "\n",
    "*You can, but do not have to, rely on the function(s) used in the lab (i.e., copying the code of the function(s))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4c671-baa8-49e3-8dc4-b86add32b952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_at_k(predictions, k, threshold):\n",
    "    '''\n",
    "    Compute precision and recall at k metrics for each user in predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: a list of (uid, iid, true_r, est, _) tuples\n",
    "    - k: the number of top items to consider\n",
    "    - threshold: the minimum true rating to consider an item as relevant\n",
    "\n",
    "    Returns:\n",
    "    - precisions: a dict mapping user ids to their precision@k score\n",
    "    - recalls: a dict mapping user ids to their recall@k score\n",
    "    '''\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions = dict()\n",
    "    recalls = dict()\n",
    "    for uid, user_ratings in user_est_true.items():\n",
    "\n",
    "        # Sort user ratings by estimated value\n",
    "        #user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        user_ratings.sort(reverse=True)\n",
    "\n",
    "        # Number of relevant and recommended items\n",
    "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
    "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
    "\n",
    "        # Number of relevant and recommended items in top k\n",
    "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
    "                              for (est, true_r) in user_ratings[:k])\n",
    "\n",
    "        # Precision@K: Proportion of recommended items that are relevant\n",
    "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
    "\n",
    "        # Recall@K: Proportion of relevant items that are recommended\n",
    "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
    "\n",
    "    return precisions, recalls\n",
    "\n",
    "\n",
    "def precision_recall_algo(algo, k_range=21, threshold=3.75):\n",
    "    '''Return precision and recall at k metrics for an algorithm.'''\n",
    "\n",
    "    # Fit algo on training set\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # Predict on test set\n",
    "    predictions = algo.test(testset)\n",
    "\n",
    "    # Compute precision and recall\n",
    "    precision = []\n",
    "    recall = []\n",
    "    for k in range(k_range):\n",
    "        precisions, recalls = precision_recall_at_k(\n",
    "            predictions, k=k, threshold=threshold)\n",
    "        precision.append(\n",
    "            sum(prec for prec in precisions.values()) / len(precisions))\n",
    "        recall.append(sum(rec for rec in recalls.values()) / len(recalls))\n",
    "\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the previously made model called knn_means and function given in the lab\n",
    "k_range, rel_threshold = 21, 3.75\n",
    "precision_KNN, recall_KNN = precision_recall_algo(knn_means, k_range, rel_threshold)\n",
    "\n",
    "# Plot precision and recall scores\n",
    "plt.plot(range(k_range), precision_KNN, 'darkblue', label='Precision')\n",
    "plt.plot(range(k_range), recall_KNN, 'red', label='Recall')\n",
    "plt.xticks(np.arange(0, k_range, step=1))\n",
    "plt.xlabel('Rank k')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.title('Precision and Recall for item-to-item based KNN')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137afeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Precision-Recall Curve\n",
    "plt.step(recall_KNN, precision_KNN, color='blue', where='post', label =f'KNN, K: {best_k}, Sim-Options:\\n{sim_options})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p pandas,numpy,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3b538-551c-4098-b82f-8c00d976a448",
   "metadata": {},
   "source": [
    "Congrats, you are done with the assignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
